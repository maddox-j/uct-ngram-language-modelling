{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa0403a-52ee-4b83-857b-65321dddf4a6",
   "metadata": {},
   "source": [
    "# Back to Basics: Let's Build a Language Model – Andrej Jovanović\n",
    "\n",
    "![title](images/llm.jpg)\n",
    "\n",
    "Large language models have taken the world by storm. Anyone heard of ChatGPT or something like that? No? Me neither…\n",
    "\n",
    "The powerful thing about these technologies are using neural networks to create successful models of language. However, language model machinery was not always so grand and complex: we once relied on basic probability theory. I find it helpful to view these language models through this probabilistic lens: it helps to understand and normalise them, in a sense. Join me in this tutorial where we create a relatively impoverished n-gram language model from scratch!\n",
    "\n",
    "*Note: this Jupyter Notebook ties in with a set of lecture slides that explain the theoretical backbone of the code. The link for which can be found here : This tutorial is based of a few resources which can be found here: \\\n",
    "https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/ \\\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf \\\n",
    "https://github.com/nltk/nltk/blob/develop/nltk/lm/__init__.py*\n",
    "\n",
    "## Step 1: What is our data?\n",
    "For this tutorial, we are going to use the famous Gutenberg corpus. We will use the NLTK package for this - it is also very useful for a wide variety of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084acc48-5c09-412a-9b19-ae6bd583293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Collect all of our imports at the top of the notebook - just for convenience ;)\n",
    "import os, nltk, string, random\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk import FreqDist, word_tokenize, sent_tokenize \n",
    "from nltk.corpus import gutenberg, stopwords, webtext\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import ngrams, pad_sequence, bigrams, everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155f820-1ed2-4fa8-aec9-7d05f2ef4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data = gutenberg.sents()\n",
    "sentence_data[480]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a4958-191f-4a5e-b07e-9d1f7b9b999d",
   "metadata": {},
   "source": [
    "As you can see, we are returned a list of lists. Where each list within the list is a sentence. These sentences actually come from novels - this sentence in particular comes from Emma by Jane Austen. Our goal now is to create a model of the English language, based on novel contained in the Gutenberg corpus. However, we first need to clean the data before we are ready to create our n-gram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3e8a9-b866-437d-9e09-3bc80e161d23",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c8245-df0b-4b16-bfdb-1f03066cfea7",
   "metadata": {},
   "source": [
    "First, we need to remove some stop words. These are words that are very frequently occuring in English and will distract from our language modelling task - bloating some of the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90eddf1-234e-47db-9eaf-1727ad152322",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'—'\n",
    "string.punctuation\n",
    "removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt']  \n",
    "def remove_stopwords(x):     \n",
    "    y = []\n",
    "    for pair in x:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word in removal_list:\n",
    "                count = count or 0\n",
    "            else:\n",
    "                count = count or 1\n",
    "        if (count==1):\n",
    "            y.append(pair)\n",
    "    return (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a92600-29a7-4ce3-8682-43001ec4c71c",
   "metadata": {},
   "source": [
    "Now, we want to build up our vocabulary. To achieve this, we need to first do a number of things \\\n",
    "- To start with, we need to tokenise our text. In this context, this means cleaning our input data so that all the words are lowercase.\n",
    "- Then, we want to create our  unigram, bigram and trigram vocabularies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcac318-6da7-45cf-8764-7c0d707fd74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate unigrams bigrams trigrams\n",
    "SOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "tokenized_text=[]\n",
    "for sentence in sentence_data:\n",
    "  sentence = list(map(lambda x:x.lower(),sentence))\n",
    "  for word in sentence:\n",
    "        if word== '.':\n",
    "            sentence.remove(word) \n",
    "        else:\n",
    "            unigram.append(word)\n",
    "  tokenized_text.append(sentence)\n",
    "  bigram.extend(list(ngrams(sentence, 2,pad_left=True, pad_right=True, left_pad_symbol = SOS, right_pad_symbol = EOS)))\n",
    "  trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True, left_pad_symbol = SOS, right_pad_symbol = EOS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3edd7b-e4c8-47c0-ab54-0e703f14c32c",
   "metadata": {},
   "source": [
    "Now let's see how frequent our tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151ba99-4998-435b-a809-703a607b80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = remove_stopwords(unigram)\n",
    "bigram = remove_stopwords(bigram)\n",
    "trigram = remove_stopwords(trigram)\n",
    "  \n",
    "# generate frequency of n-grams \n",
    "freq_bi = FreqDist(bigram)\n",
    "freq_tri = FreqDist(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7dd219-c343-4987-8457-cdcdfe5cac65",
   "metadata": {},
   "source": [
    "Now, let's try and generate some sentences from our tri-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea240d8-6f56-463b-ae3a-e9730109d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for a, b, c in freq_tri:\n",
    "    if(a != None and b!= None and c!= None):\n",
    "        if (a,b) not in d.keys():\n",
    "            d[a,b] = {}\n",
    "        if c not in d[a,b].keys():\n",
    "            d[a,b][c] = freq_tri[a,b,c]\n",
    "        else:\n",
    "            d[a,b][c] += freq_tri[a,b,c]\n",
    "# Next word prediction    \n",
    "def pick_word(counter):\n",
    "    \"Chooses a random element.\"\n",
    "    return random.choice(list(counter.items()))\n",
    "s = [SOS,SOS]\n",
    "num_tokens = 20\n",
    "for i in range(num_tokens):\n",
    "    if s[-2] == EOS and s[-1] == EOS:\n",
    "        break\n",
    "    prefix = (s[-2], s[-1])\n",
    "    suffix = pick_word(d[prefix])\n",
    "    s.append(suffix[0])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919fc73a-89ec-4a82-86b9-a17b99755791",
   "metadata": {},
   "source": [
    "As we can see, we have some output. In some cases it is fluent, but in most cases it isn't. Why do we think that is? Do you notice anything interesting about the type of sentences that the model is generating?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503d4f4-6a37-4b5d-8b74-fb35e105d42b",
   "metadata": {},
   "source": [
    "## Let's step it up a bit: Let's use NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2248256-650f-4f0d-9bc2-12b3c84a309d",
   "metadata": {},
   "source": [
    "NLTK fortunately has a lot of this functionality baked into it. Let's trying using their classes and methods on different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b418c-78fd-4892-8332-54360e08a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data = webtext.sents()\n",
    "tokenized_text=[]\n",
    "for sentence in sentence_data:\n",
    "  sentence = list(map(lambda x:x.lower(),sentence))\n",
    "  for word in sentence:\n",
    "        if word== '.':\n",
    "            sentence.remove(word) \n",
    "  tokenized_text.append(sentence)\n",
    "tokenized_text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a930ec-5609-4461-8382-f804be3ad41d",
   "metadata": {},
   "source": [
    "Let's generate our sentences and our data for our tri-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b027c-4f25-4c55-8e6b-19b4d5091315",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7af4e-a2a2-4ac7-ae64-c87149b35910",
   "metadata": {},
   "source": [
    "Now let's fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cacf5-93e6-4c23-9887-72c90b0fba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1dad6-0b82-41af-bd27-a6de77a9d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, padded_sents)\n",
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d5c91-742b-4e11-bb74-17d2007304ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocab.lookup(tokenized_text[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d13939-ed1a-41d9-a5f3-53d7659f7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocab.lookup([\"we\", \"want\", \"chatgpt\", \"to\", \"do\", \"my\", \"homework\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df67865-e95f-4cc7-84ec-6af58aafac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generate(20, random_seed=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245aa847-a769-49f0-b3df-2a77bdac39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17951028-c523-4747-bbda-15f277a08ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e85262-18ea-4ec0-b904-3e210110266b",
   "metadata": {},
   "source": [
    "## But wait there is more!\n",
    "\n",
    "Did you notice something strange when we were creating our language model? We imported something called MLE – that stands for maximum likelihood estimation. This is a method of creating our language model where we estimate our probabilities directly from the data. Furthermore, in our case, we set our vocab cuttoff to 1: we wanted to ignore words that didn't occur frequently enough to provide us useful information. But what happens if we want to interpret the probability of a sentence that has such a word that we do not know in our vocabulary? Should the probability immediately be 0?\n",
    "\n",
    "I think not. This problem is known as smoothing n-gram models! \n",
    "\n",
    "To find examples of smoothing, look at [these objects from `nltk.lm.models`](https://github.com/nltk/nltk/blob/develop/nltk/lm/models.py):\n",
    "\n",
    " - `Lidstone`: Provides Lidstone-smoothed scores.\n",
    " - `Laplace`: Implements Laplace (add one) smoothing.\n",
    " - `InterpolatedLanguageModel`: Logic common to all interpolated language models (Chen & Goodman 1995).\n",
    " - `WittenBellInterpolated`: Interpolated version of Witten-Bell smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eca3fa-e5f8-4c92-ac45-147b3ea003ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - IMPLEMENT YOUR OWN SMOOTHED N-GRAM MODELS!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
