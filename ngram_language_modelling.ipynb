{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa0403a-52ee-4b83-857b-65321dddf4a6",
   "metadata": {},
   "source": [
    "# Back to Basics: Let's Build a Language Model – Andrej Jovanović\n",
    "\n",
    "![title](images/llm.jpg)\n",
    "\n",
    "Large language models have taken the world by storm. Anyone heard of ChatGPT or something like that? No? Me neither…\n",
    "\n",
    "The powerful thing about these technologies is that they are using neural networks to create successful models of language. However, language model machinery was not always so grand and complex: we once relied on basic probability theory. I find it helpful to view these language models through this probabilistic lens: it helps to understand and normalise them, in a sense. Join me in this tutorial where we create a relatively impoverished n-gram language model from scratch!\n",
    "\n",
    "*Note: this Jupyter Notebook ties in with a set of lecture slides that explain the theoretical backbone of the code. The link for which can be found here : This tutorial is based of a few resources which can be found here: \\\n",
    "https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/ \\\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf \\\n",
    "https://github.com/nltk/nltk/blob/develop/nltk/lm/__init__.py*\n",
    "\n",
    "## Step 1: What is our data?\n",
    "For this tutorial, we are going to use the famous Gutenberg corpus (a dataset that just contains a lot of text). We will use the NLTK package for this which is very useful for a wide variety of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084acc48-5c09-412a-9b19-ae6bd583293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all of our imports at the top of the notebook - just for convenience ;)\n",
    "import os, nltk, string, random\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk import FreqDist, word_tokenize, sent_tokenize \n",
    "from nltk.corpus import gutenberg, stopwords, webtext\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import ngrams, pad_sequence, bigrams, everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155f820-1ed2-4fa8-aec9-7d05f2ef4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO - view data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a4958-191f-4a5e-b07e-9d1f7b9b999d",
   "metadata": {},
   "source": [
    "As you can see, we are returned a list of lists. Where each list within the list is a sentence. These sentences actually come from novels - this sentence in particular comes from Emma by Jane Austen. Our goal now is to create a model of the English language, based on novel contained in the Gutenberg corpus. However, we first need to clean the data before we are ready to create our n-gram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3e8a9-b866-437d-9e09-3bc80e161d23",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c8245-df0b-4b16-bfdb-1f03066cfea7",
   "metadata": {},
   "source": [
    "First, we need to remove some stop words. These are words that are very frequently occuring in English and will distract from our language modelling task - bloating some of the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90eddf1-234e-47db-9eaf-1727ad152322",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a92600-29a7-4ce3-8682-43001ec4c71c",
   "metadata": {},
   "source": [
    "Now, we want to build up our vocabulary. To achieve this, we need to first do a number of things \\\n",
    "- To start with, we need to tokenise our text. In this context, this means cleaning our input data so that all the words are lowercase.\n",
    "- Then, we want to create our  unigram, bigram and trigram vocabularies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcac318-6da7-45cf-8764-7c0d707fd74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3edd7b-e4c8-47c0-ab54-0e703f14c32c",
   "metadata": {},
   "source": [
    "Now let's see how frequent our tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151ba99-4998-435b-a809-703a607b80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7dd219-c343-4987-8457-cdcdfe5cac65",
   "metadata": {},
   "source": [
    "Now, let's try and generate some sentences from our tri-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea240d8-6f56-463b-ae3a-e9730109d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919fc73a-89ec-4a82-86b9-a17b99755791",
   "metadata": {},
   "source": [
    "As we can see, we have some output. In some cases it is fluent, but in most cases it isn't. Why do we think that is? Do you notice anything interesting about the type of sentences that the model is generating?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503d4f4-6a37-4b5d-8b74-fb35e105d42b",
   "metadata": {},
   "source": [
    "## Let's step it up a bit: Let's use NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2248256-650f-4f0d-9bc2-12b3c84a309d",
   "metadata": {},
   "source": [
    "NLTK fortunately has a lot of this functionality baked into it. Let's trying using their classes and methods on different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b418c-78fd-4892-8332-54360e08a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a930ec-5609-4461-8382-f804be3ad41d",
   "metadata": {},
   "source": [
    "Let's generate our sentences and our data for our tri-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b027c-4f25-4c55-8e6b-19b4d5091315",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7af4e-a2a2-4ac7-ae64-c87149b35910",
   "metadata": {},
   "source": [
    "Now let's fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cacf5-93e6-4c23-9887-72c90b0fba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1dad6-0b82-41af-bd27-a6de77a9d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d5c91-742b-4e11-bb74-17d2007304ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d13939-ed1a-41d9-a5f3-53d7659f7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df67865-e95f-4cc7-84ec-6af58aafac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245aa847-a769-49f0-b3df-2a77bdac39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17951028-c523-4747-bbda-15f277a08ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e85262-18ea-4ec0-b904-3e210110266b",
   "metadata": {},
   "source": [
    "## But wait there is more!\n",
    "\n",
    "Did you notice something strange when we were creating our language model? We imported something called MLE – that stands for maximum likelihood estimation. This is a method of creating our language model where we estimate our probabilities directly from the data. Furthermore, in our case, we set our vocab cuttoff to 1: we wanted to ignore words that didn't occur frequently enough to provide us useful information. But what happens if we want to interpret the probability of a sentence that has such a word that we do not know in our vocabulary? Should the probability immediately be 0?\n",
    "\n",
    "I think not. This problem is known as smoothing n-gram models! \n",
    "\n",
    "To find examples of smoothing, look at [these objects from `nltk.lm.models`](https://github.com/nltk/nltk/blob/develop/nltk/lm/models.py):\n",
    "\n",
    " - `Lidstone`: Provides Lidstone-smoothed scores.\n",
    " - `Laplace`: Implements Laplace (add one) smoothing.\n",
    " - `InterpolatedLanguageModel`: Logic common to all interpolated language models (Chen & Goodman 1995).\n",
    " - `WittenBellInterpolated`: Interpolated version of Witten-Bell smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eca3fa-e5f8-4c92-ac45-147b3ea003ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - IMPLEMENT YOUR OWN SMOOTHED N-GRAM MODELS!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
